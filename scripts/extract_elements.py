import os
import re
import json
import hashlib
from collections import Counter
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer

def leer_archivo(ruta_archivo):
    encodings = ['utf-8', 'latin-1', 'ISO-8859-1']
    for encoding in encodings:
        try:
            with open(ruta_archivo, 'r', encoding=encoding) as file:
                return file.read()
        except UnicodeDecodeError:
            continue
    print(f"Error: No se pudo leer el archivo {ruta_archivo} con ninguna codificaci√≥n conocida.")
    return None

def buscar_carpeta_proyecto_visual_studio(ruta_src):
    for carpeta in os.listdir(ruta_src):
        ruta_carpeta = os.path.join(ruta_src, carpeta)
        if os.path.isdir(ruta_carpeta):
            for archivo in os.listdir(ruta_carpeta):
                if archivo.endswith(('.cpp', '.h')):
                    return ruta_carpeta
    return ruta_src

def extraer_elementos(contenido):
    variables = re.findall(r'\b(?:int|float|double|char|bool|string)\s+(\w+)', contenido)
    variables += re.findall(r'\bthis->(\w+)', contenido)
    funciones = re.findall(r'\b(\w+)\s*\([^)]*\)\s*{', contenido)
    clases = re.findall(r'\bclass\s+(\w+)', contenido)
    comentarios = re.findall(r'//.*?$|/\*.*?\*/', contenido, re.DOTALL | re.MULTILINE)
    estructuras_control = re.findall(r'\b(if|else|for|while|switch|case)\b', contenido)
    llamadas_funciones = re.findall(r'\b(\w+)\s*\(', contenido)
    tipos_datos = re.findall(r'\b(int|float|double|char|bool|string|auto)\b', contenido)
    operadores = re.findall(r'[+\-*/%=<>!&|^~]', contenido)
    
    return {
        'variables': variables,
        'funciones': funciones,
        'clases': clases,
        'comentarios': [c.strip() for c in comentarios],
        'estructuras_control': estructuras_control,
        'llamadas_funciones': llamadas_funciones,
        'tipos_datos': tipos_datos,
        'operadores': operadores
    }

def calcular_hash(texto):
    return hashlib.md5(texto.encode()).hexdigest()

def calcular_metricas_codigo(contenido):
    lineas = contenido.split('\n')
    return {
        'total_lineas': len(lineas),
        'lineas_codigo': len([l for l in lineas if l.strip() and not l.strip().startswith('//')]),
        'lineas_comentarios': len([l for l in lineas if l.strip().startswith('//')]),
        'lineas_vacias': len([l for l in lineas if not l.strip()]),
    }

def analizar_complejidad(contenido):
    return len(re.findall(r'\b(if|for|while|switch)\b', contenido))

def extraer_secuencias(contenido, longitud=3):
    tokens = re.findall(r'\b\w+\b|[+\-*/%=<>!&|^~;{}()\[\]]', contenido)
    return [' '.join(tokens[i:i+longitud]) for i in range(len(tokens) - longitud + 1)]

def analizar_archivos(ruta_src):
    resultados = {}
    todos_elementos = []
    todos_secuencias = []
    
    ruta_carpeta_proyecto = buscar_carpeta_proyecto_visual_studio(ruta_src)

    for raiz, _, ficheros in os.walk(ruta_carpeta_proyecto):
        for fichero in ficheros:
            if fichero.endswith(('.cpp', '.h')):
                ruta_completa = os.path.join(raiz, fichero)
                contenido = leer_archivo(ruta_completa)
                if contenido is None:
                    continue
                
                elementos = extraer_elementos(contenido)
                todos_elementos.append(' '.join(elementos['variables'] + elementos['funciones'] +
                                                elementos['clases'] + elementos['comentarios']))
                
                secuencias = extraer_secuencias(contenido)
                todos_secuencias.extend(secuencias)
                
                metricas = calcular_metricas_codigo(contenido)
                complejidad = analizar_complejidad(contenido)
                
                resultados[fichero] = {
                    'elementos': elementos,
                    'hash_contenido': calcular_hash(contenido),
                    'metricas': metricas,
                    'complejidad': complejidad,
                    'estadisticas': {
                        'num_variables': len(elementos['variables']),
                        'num_funciones': len(elementos['funciones']),
                        'num_clases': len(elementos['clases']),
                        'num_comentarios': len(elementos['comentarios']),
                        'freq_estructuras_control': dict(Counter(elementos['estructuras_control'])),
                        'freq_llamadas_funciones': dict(Counter(elementos['llamadas_funciones'])),
                        'freq_tipos_datos': dict(Counter(elementos['tipos_datos'])),
                        'freq_operadores': dict(Counter(elementos['operadores']))
                    },
                    'secuencias_comunes': Counter(secuencias).most_common(10)
                }
    
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(todos_elementos)
    
    for i, (fichero, datos) in enumerate(resultados.items()):
        datos['vector_tfidf'] = tfidf_matrix[i].toarray()[0].tolist()
    
    estadisticas_globales = {
        'total_archivos': len(resultados),
        'total_lineas': sum(datos['metricas']['total_lineas'] for datos in resultados.values()),
        'total_funciones': sum(datos['estadisticas']['num_funciones'] for datos in resultados.values()),
        'total_clases': sum(datos['estadisticas']['num_clases'] for datos in resultados.values()),
        'complejidad_promedio': sum(datos['complejidad'] for datos in resultados.values()) / len(resultados) if resultados else 0,
        'secuencias_mas_comunes': Counter(todos_secuencias).most_common(20)
    }
    
    return {
        'archivos': resultados,
        'estadisticas_globales': estadisticas_globales
    }

def guardar_resultados(resultados, ruta_salida):
    with open(ruta_salida, 'w', encoding='utf-8') as f:
        json.dump(resultados, f, indent=2, ensure_ascii=False)

def generar_reporte_md(resultados):
    estadisticas = resultados['estadisticas_globales']
    md = f"# üìä Reporte de An√°lisis de C√≥digo\n\n"
    md += f"üìÖ Fecha de generaci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    md += "## üìà Estad√≠sticas Globales\n\n"
    md += f"- üìÅ Total de archivos analizados: **{estadisticas['total_archivos']}**\n"
    md += f"- üìù Total de l√≠neas de c√≥digo: **{estadisticas['total_lineas']}**\n"
    md += f"- üîß Total de funciones: **{estadisticas['total_funciones']}**\n"
    md += f"- üèóÔ∏è Total de clases: **{estadisticas['total_clases']}**\n"
    md += f"- üîÑ Complejidad promedio: **{estadisticas['complejidad_promedio']:.2f}**\n\n"

    md += "### üîÅ Secuencias m√°s comunes\n\n"
    for seq, count in estadisticas['secuencias_mas_comunes'][:5]:
        md += f"- `{seq}`: {count} veces\n"

    return md

def main():
    print("üîç Iniciando an√°lisis de c√≥digo...")
    ruta_proyecto = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    ruta_src = os.path.join(ruta_proyecto, 'src')
    ruta_output = os.path.join(ruta_proyecto, 'output')
    os.makedirs(ruta_output, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    ruta_resultados = os.path.join(ruta_output, f'caracteristicas_a_comparar_{timestamp}.json')
    ruta_reporte = os.path.join(ruta_output, f'reporte_analisis_{timestamp}.md')

    print("üìä Analizando archivos...")
    resultados = analizar_archivos(ruta_src)
    
    print("üíæ Guardando resultados detallados...")
    guardar_resultados(resultados, ruta_resultados)

    print("üìù Generando reporte resumido...")
    reporte_md = generar_reporte_md(resultados)
    with open(ruta_reporte, 'w', encoding='utf-8') as f:
        f.write(reporte_md)

    print(f"‚úÖ An√°lisis completado.")
    print(f"üìä Resultados detallados guardados en: {ruta_resultados}")
    print(f"üìë Reporte resumido guardado en: {ruta_reporte}")

if __name__ == "__main__":
    main()
